{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c01ad180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8395747",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd4ffb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1c64d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder = \"../backend-project/data/\", filenames_to_keep=None):\n",
    "    '''\n",
    "    Load the data about each image, the names of the images used for training, the name of the ones used for testing\n",
    "    and the bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : optional, str\n",
    "        The folder where the data is stored\n",
    "    filenames_to_keep : optional, np.array(M,)\n",
    "        List of all the filenames we want to keep, the other will be filtered. Default is None, meaning we \n",
    "        want to keep all the data in the folder.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    data, train_val_filenames, test_filenames, bounding_boxes: \n",
    "    pd.DataFrame(N, 11), np.array(V, ), np.array(T, ),  pd.DataFrame(B, 6)\n",
    "        The data contains the info on the image such as disease, patient id/age, resolution.\n",
    "        The train_val_filenames contains the name of the images used for training/validation\n",
    "        The test_filenames contains the name of the images used for testing\n",
    "        The bounding_boxes contains the info on the bboxes of known diseases which are the name of the file\n",
    "        and the disease, the x,y coordinates of the top left corner and the width and height of the box.\n",
    "    '''\n",
    "    data = pd.read_csv(folder + \"Data_Entry_2017_v2020.csv\")\n",
    "    train_val_filenames = np.array(pd.read_csv(folder + \"train_val_list.txt\", names=[\"filename\"])['filename'].tolist())\n",
    "    test_filenames = np.array(pd.read_csv(folder + \"test_list.txt\", names=[\"filename\"])['filename'].tolist())\n",
    "    bounding_boxes = pd.read_csv(folder + \"BBox_List_2017.csv\")\n",
    "    \n",
    "    data = data.rename(columns={\"Image Index\": \"Filename\", \"Finding Labels\": \"Diseases\", \"OriginalImage[Width\":\"Original Width\", \"Height]\": \"Original Height\", \"OriginalImagePixelSpacing[x\": \"Original Pixel Spacing x\", \"y]\": \"Original Pixel Spacing y\"})\n",
    "    bounding_boxes=bounding_boxes.loc[:, ~bounding_boxes.columns.str.contains('^Unnamed')].rename(columns={\"Image Index\": \"Filename\", \"Finding Label\": \"Disease\", \"Bbox [x\" : \"x\", \"h]\": \"h\"})\n",
    "    \n",
    "    if filenames_to_keep is not None:\n",
    "        data = data[data[\"Filename\"].isin(filenames_to_keep)]\n",
    "        bounding_boxes = bounding_boxes[bounding_boxes[\"Filename\"].isin(filenames_to_keep)]\n",
    "        test_filenames = test_filenames[np.isin(test_filenames, filenames_to_keep)]\n",
    "        train_val_filenames = train_val_filenames[np.isin(train_val_filenames, filenames_to_keep)]\n",
    "        \n",
    "    return data, train_val_filenames, test_filenames, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b92a6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    '''\n",
    "    Read an image in memory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path to the image (folder + filename)\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    np_img: np.array(X, Y):\n",
    "        The image (not scaled) stored in this path\n",
    "    '''\n",
    "    img = Image.open(path)\n",
    "    np_img = np.array(img)\n",
    "\n",
    "    return np_img\n",
    "\n",
    "def scale_image(np_img):\n",
    "    '''\n",
    "    Scale an image from range [0, 255] to range [0, 1]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    np_img : np.array(X, Y)\n",
    "        The unscaled ([0, 255]) image\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    np.array(X, Y):\n",
    "        The scaled ([0, 1]) image\n",
    "    '''\n",
    "    return np_img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e562d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    '''\n",
    "    Class to represent our dataset of XRay images. Subclass of torch.utils.data.Dataset. Store the image directory,\n",
    "    images name, bounding boxes and data of the images we will use either for training or testing.\n",
    "    '''\n",
    "    def __init__(self, img_dir, transform=None, train=True):\n",
    "        '''\n",
    "        Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_dir : str\n",
    "            Directory where the images are stored. Will take all the png files present in this directory\n",
    "        transform: optional, function\n",
    "            Transformation used on every image before returning it such as scaling.\n",
    "        train: optional, Boolean\n",
    "            If set to true, we keep only the training data in the corresponding folder and otherwise the testing one\n",
    "        '''\n",
    "        self.img_dir = img_dir\n",
    "        # Read all the names of the PNG files in the directory\n",
    "        images_name = np.array([name for name in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, name)) and re.search('png$', name) is not None])\n",
    "        data, train_names, test_names, bounding_boxes = load_data(filenames_to_keep = images_name)\n",
    "        \n",
    "        # Keep only the training or test ones\n",
    "        self.data = data[data[\"Filename\"].isin((train_names if train else test_names))]\n",
    "        self.bounding_boxes = bounding_boxes[bounding_boxes[\"Filename\"].isin((train_names if train else test_names))]\n",
    "        self.images_name = images_name[np.isin(images_name, (train_names if train else test_names))]\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Overload: return the length of the dataset\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        length: int:\n",
    "            The length of the dataset\n",
    "        '''\n",
    "        return self.images_name.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Overload; get the item (only image for the moment) corresponding to the given idx.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the item we want to get.\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        image: np.array(X, Y):\n",
    "            The corresponding image transformed with the transform method\n",
    "        '''\n",
    "        img_path = os.path.join(self.img_dir, self.images_name[idx])\n",
    "        image = read_image(img_path)\n",
    "        \n",
    "        if(len(image.shape) > 2): #Some images have more than one channel for unknown reasons\n",
    "            image = image[:, :, 0]\n",
    "\n",
    "        data = self.data[self.data[\"Filename\"] == self.images_name[idx]]\n",
    "        \n",
    "        bbox = None\n",
    "        if not self.bounding_boxes[self.bounding_boxes[\"Filename\"] == self.images_name[idx]].empty:\n",
    "            bbox = self.bounding_boxes[self.bounding_boxes[\"Filename\"] == self.images_name[idx]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "#             if image.shape[0] == 1:\n",
    "#                 image = image.squeeze()\n",
    "\n",
    "        return image#, data, bbox # Cannot pass data and bbox for the moment because they contain entries of type \"object\" which is a problem for dataloader. Should transform the few entries having object type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057c5ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa15201",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233a811",
   "metadata": {},
   "source": [
    "## Variational Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c91b4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4, 16, 32, 64, 128]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*32*32, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*32*32, latent_dim)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c315d3",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd70a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_in_channels = encoder_in_channels\n",
    "        \n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [128, 64, 32, 16, 4]\n",
    "\n",
    "        # Build Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[0] * 32 * 32)\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels=encoder_in_channels,\n",
    "                                      kernel_size=3, padding=1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 128, 32, 32)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410619b3",
   "metadata": {},
   "source": [
    "## Beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c6d93bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims_encoder: List = None,\n",
    "                 hidden_dims_decoder: List = None,\n",
    "                 beta: int = 4,\n",
    "                 gamma:float = 1000.,\n",
    "                 max_capacity: int = 25,\n",
    "                 Capacity_max_iter: int = 1e5,\n",
    "                 loss_type:str = 'B',\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "        self.num_iter = 0\n",
    "        \n",
    "        self.variational_encoder = VariationalEncoder(in_channels, latent_dim, hidden_dims_encoder)\n",
    "        self.decoder = Decoder(in_channels, latent_dim, hidden_dims_decoder)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        mu, log_var = self.variational_encoder(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decoder(z), input, mu, log_var]\n",
    "    \n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        self.num_iter += 1\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            loss = recons_loss + self.beta * kld_loss\n",
    "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
    "            self.C_max = self.C_max.to(input.device)\n",
    "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
    "            loss = recons_loss + self.gamma * (kld_loss - C).abs()\n",
    "        else:\n",
    "            raise ValueError('Undefined loss type.')\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35959d94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce8562",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12b0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, num_epochs=5, verbose=1):\n",
    "    '''\n",
    "    Train the model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        The model.\n",
    "    train_loader: DataLoader\n",
    "        A Pytorch Dataloader containing the training images.\n",
    "    num_epochs: optional, int \n",
    "        The number of epochs to train.\n",
    "    verbose: optional, int\n",
    "        Set the number of printing we want during training. For the moment, only 0 for no printing or another value\n",
    "        to get some information about the update.\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    loss_history: list\n",
    "        A list of training losses. One for each epoch of training.\n",
    "    '''\n",
    "    loss_history = []\n",
    "    for e in range(num_epochs):\n",
    "        if verbose:\n",
    "            print(f\"=========== EPOCH {e} ===========\")\n",
    "        total_loss = 0\n",
    "        number_samples = 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            X = batch\n",
    "            output = model(X)\n",
    "            loss = loss_fn(*output)\n",
    "            total_loss += loss[\"loss\"].detach()\n",
    "            number_samples += X.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss[\"loss\"].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if verbose and not i%100 and not i==0:\n",
    "                print(f\"Done processing batch number {i}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Mean loss = {total_loss/number_samples}\")\n",
    "        \n",
    "        loss_history.append(total_loss/number_samples)\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f7229a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(epoch_losses, title='Loss'):\n",
    "    '''\n",
    "    Simple plot for the loss at each epoch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch_losses: list\n",
    "        A list of training losses. One for each epoch of training.\n",
    "    title: optional, str\n",
    "        the title of the plot.\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c2215f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dataloader for training and testing using our custom dataset\n",
    "IMG_DIR = \"../backend-project/data/images/images/\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "training_data = XRayDataset(\n",
    "    img_dir=IMG_DIR,\n",
    "    train=True,\n",
    "    transform=ToTensor() #This method directly scale the image in [0, 1] range\n",
    ")\n",
    "\n",
    "test_data = XRayDataset(\n",
    "    img_dir=IMG_DIR,\n",
    "    train=False,\n",
    "    transform=ToTensor() #This method directly scale the image in [0, 1] range\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(training_data,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_data, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "123ce77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== EPOCH 0 ===========\n",
      "Done processing batch number 100\n",
      "Done processing batch number 200\n",
      "Done processing batch number 300\n",
      "Done processing batch number 400\n",
      "Done processing batch number 500\n",
      "Done processing batch number 600\n",
      "Done processing batch number 700\n",
      "Done processing batch number 800\n",
      "Done processing batch number 900\n",
      "Done processing batch number 1000\n",
      "Done processing batch number 1100\n",
      "Done processing batch number 1200\n",
      "Done processing batch number 1300\n",
      "Done processing batch number 1400\n",
      "Done processing batch number 1500\n",
      "Done processing batch number 1600\n",
      "Done processing batch number 1700\n",
      "Done processing batch number 1800\n",
      "Done processing batch number 1900\n",
      "Done processing batch number 2000\n",
      "Mean loss = tensor([nan])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/miniconda3/envs/xai/lib/python3.10/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create our autoencoder and train it\n",
    "# Parameters\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "LATENT_SPACE_DIM = 16\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "beta_vae = BetaVAE(in_channels=1, latent_dim=16)\n",
    "\n",
    "# Loss function & optimizer\n",
    "loss_fn = beta_vae.loss_function\n",
    "optimizer = optim.Adam(beta_vae.parameters(),\n",
    "                       lr=LEARNING_RATE)\n",
    "\n",
    "epoch_losses = train(beta_vae, train_loader, num_epochs=NUM_EPOCHS)\n",
    "plot_loss(epoch_losses, 'Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "be252cf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BetaVAE' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbeta_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36mBetaVAE.sample\u001b[0;34m(self, num_samples, current_device, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_samples,\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n\u001b[1;32m     78\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mto(current_device)\n\u001b[0;32m---> 80\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(z)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BetaVAE' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "image_generated = beta_vae.generate(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a711d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
